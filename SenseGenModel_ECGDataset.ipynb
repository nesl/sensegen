{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Moustafa Alzantot (malzantot@ucla.edu)\n",
    "All rights reserved Networked and Embedded Systems Lab (NESL), UCLA.\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import model_utils\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_utils.load_training_data('ecg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get reasonable outputs, should use something bigger than 1000 !\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.reset_session_and_model()\n",
    "with tf.Session() as sess:\n",
    "    train_config = model.ModelConfig()\n",
    "    test_config = model.ModelConfig()\n",
    "    train_config.learning_rate = 0.003\n",
    "    train_config.num_layers = 1\n",
    "    train_config.batch_size = 128\n",
    "    test_config.num_layers = 1\n",
    "    test_config.batch_size = 1\n",
    "    test_config.num_steps = 1\n",
    "    loader = data_utils.DataLoader(data=data,batch_size=train_config.batch_size, num_steps=train_config.num_steps)\n",
    "    train_model = model.MDNModel(train_config, True)\n",
    "    test_model = model.MDNModel(test_config, False)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for idx in range(num_epochs):\n",
    "        epoch_loss = train_model.train_for_epoch(sess, loader)\n",
    "        print(idx, ' ', epoch_loss)\n",
    "        if (idx+1) % 100 == 0:\n",
    "            saver.save(sess, './models/ecg_mdnmodel.ckpt', global_step=idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sampling from a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = './models/ecg_mdnmodel.ckpt-999'\n",
    "seq_len = 1200\n",
    "model_utils.reset_session_and_model()\n",
    "true_data = data[0,:seq_len]\n",
    "with tf.Session() as sess:\n",
    "    test_config = model.ModelConfig()\n",
    "    test_config.num_layers = 1\n",
    "    test_config.batch_size = 1\n",
    "    test_config.num_steps = 1\n",
    "    test_model = model.MDNModel(test_config, True)\n",
    "    test_model.is_training = False\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, ckpt_path)\n",
    "    fake_data = test_model.predict(sess, seq_len)\n",
    "fig, axes = plt.subplots(1,2, figsize=((14,8)))\n",
    "axes[0].plot(true_data)\n",
    "axes[0].set_title('True data')\n",
    "axes[1].plot(fake_data)\n",
    "axes[1].set_title('Fake data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note:\n",
    "# I didn't spend time to do hyperparameter selection for the model and improve the results.\n",
    "# Changing the number of Guassian mixtures, number of hidden units , number of epochs, or learning rate should improve the quality of results.\n",
    "# Another useful trick would be to 'quantize' the signal into discrete set of levels and use one-hot-encoding for input processing and cross-entropy loss.\n",
    "# Under such a setting, the model will be similar to the RNNLM architecture trained using maximum likelihood estimate (MLE).\n",
    "# Sometimes the model above makes a mistakce and produce signal that not looking well-shaped, this is mainly due to that MLE-based models\n",
    "# suffer from `exposure bias` which means at generation time the model may produce data different from those seen during training time, and since\n",
    "# the model relies on its own prediction as an input for next step, the error would propagate to future time steps as well.\n",
    "# Further direction of improvement would be pairing the generator model with a discriminator and train the generator by adversarial loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
